---
output:
  pdf_document:
    fig_caption: true
  word_document:
    fig_caption: true
header-includes:
   - \usepackage{booktabs}
   - \usepackage[final]{changes}
   - \usepackage[font={small},labelfont=bf,labelsep=colon]{caption}
   - \linespread{1.5}
   - \usepackage{enumitem}
   - \usepackage{tikz}
   - \def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
   - \setlist{nolistsep}
   - \setremarkmarkup{(#2)}
bibliography: references.bib
csl: national-science-foundation-grant-proposals.csl
fontsize: 11pt
mainfont: Georgia
geometry: margin=1.0in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( cache=TRUE )
```

\pagenumbering{gobble}


<!--
---
output:
  word_document:
    fig_caption: true
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
    keep_tex: yes
  html_document:
    toc: false
header-includes:
   - \usepackage{booktabs}
   - \usepackage[final]{changes}
   - \usepackage[font={small},labelfont=bf,labelsep=colon]{caption}
   - \linespread{1.2}
   - \usepackage[compact]{titlesec}
   - \usepackage{enumitem}
   - \usepackage{tikz}
   - \def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
   - \setlist{nolistsep}
   - \setremarkmarkup{(#2)}
bibliography: references.bib
csl: national-science-foundation-grant-proposals.csl
fontsize: 11pt
mainfont: Georgia
geometry: margin=1.0in
---
-->

\begin{centering}

$ $

\vspace{2.0 cm}

\LARGE

{\bf The ANTs Longitudinal Cortical Thickness Pipeline}

\vspace{0.25 cm}

\large
Nicholas J. Tustison$^{1,2}$,
Andrew J. Holbrook$^3$,
Brian B. Avants$^4$,
Jared M. Roberts$^2$,
Philip A. Cook$^5$,
James R. Stone$^1$,
Daniel L. Gillen$^3$, and
Michael A. Yassa$^2$
for the Alzheimer's Disease Neuroimaging Initiative*


\vspace{0.5 cm}

\small

$^1$Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA

$^2$Department of Neurobiology and Behavior, University of California, Irvine, Irvine, CA

$^3$Department of Statistics, University of California, Irvine, Irvine, CA

$^4$Biogen, Cambridge, MA

$^5$Department of Radiology, University of Pennsylvania, Philadelphia, PA

\end{centering}

\vspace{4.0 cm}

\small

Corresponding author: \
Nicholas J. Tustison \
211 Qureshey Research Lab \
Irvine, CA  92697-3800 \
540-383-2719 \
ntustison@virginia.edu \

\noindent\rule{4cm}{0.4pt}

\footnotesize
*Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf

\newpage

\normalsize

# Abstract

Large-scale longitudinal studies of developmental progression or disease in the human brain
have motivated the acquisition of large neuroimaging data sets and the
concomitant development of robust methodological and statistical tools
for insight into potential neurostructural changes.  Longitudinal-specific strategies
for acquisition and processing have potentially significant benefits including
the reduction of the inter-subject confound associated with cross-sectional
studies.  In this work, we introduce the open-source Advanced Normalization Tools
(ANTs) cortical thickness longitudinal processing pipeline and its application
on the first phase of the Alzheimer's Disease Neuroimaging Initiative (ADNI-1)
consisting of over 600 subjects with multiple time points from baseline to 36 months.
We demonstrate that the single-subject template construction and native subject-space
processing localizes data transformations and reduces interpolation artifacts, respectively,
and is the preferred processing strategy with respect to simultaneous minimization
of within-subject variability and maximization of between-subject variability, respectively.
It is further shown that optimizing these dual criteria
leads to greater scientific interpretability in terms of tighter confidence intervals
in calculated mean trends, smaller prediction intervals,
and tighter confidence/credible intervals for determining cross-sectional effects.

\clearpage

<!--

You can add internal comments which will not be reproduced using html comment delimiters.

-->

# Introduction

Quantification of brain morphology significantly facilitates the investigation of
a wide range of neurological conditions with structural correlates (e.g.,
Alzheimer's disease and frontotemporal dementia [@du2007;@dickerson2009],
Parkinson's disease [@jubault2011],
Williams syndrome [@thompson2005],
multiple sclerosis [@ramasamy2009],
autism [@chung2005,@hardan2006],
migraines [@dasilva2007],
chronic smoking [@kuhn2010],
alcoholism [@fortier2011],
cocaine addiction [@makris2008],
schizophrenia [@nesvag2008],
bipolar disorder [@lyoo2006],
autism [@chung2005,@hardan2006],
marijuana use in adolescents [@Jacobus:2015aa],
Tourette syndrome in children [@sowell2008],
scoliosis in female adolescents [@wang2012],
heart failure [@Kumar:2015aa],
early-onset blindness [@jiang2009],
chronic pancreatitis [@frokjaer2012],
obsessive-compulsive disorder [@shin2007],
ADHD [@almeida-montes2012],
obesity [@raji2010],
heritable [@peterson2009] and elderly [@ballmaier2004] depression,
age [@kochunov2011],
gender [@luders2006a],
untreated male-to-female transsexuality [@luders2012],
handedness
[@luders2006,amunts2007],
intelligence [@shaw2006],
athletic ability [@wei2011],
meditative practices [@lazar2005],
musical ability [@bermudez2009;@foster2010],
musical instrument playing [@Hudziak:2014aa],
tendency toward criminality [@raine2011],
childhood sexual abuse in adult females [@heim2013],
and Tetris-playing ability in female adolescents [@haier2009]).
Essential for thickness quantification are the
many computational techniques which have been developed
to provide accurate measurements of the cerebral cortex.
These include various mesh-based
(e.g., [@macdonald2000;@magnotta1999;@kim2005]) and
volumetric techniques
(e.g., [@zeng1999;@jones2000;@das2009;@clement-vachet2011]).
Of noted significance, and representing the former,
is the well-known and highly utilized FreeSurfer
software package [@dale1999;@fischl1999;@fischl2000;@fischl2002;@fischl2004].

In inferring developmental processes, many of these studies employ
cross-sectional population sampling strategies despite the potential for
confounding effects [@Kraemer:2000aa].  Large-scale studies involving longitudinal
image acquisition of a targeted subject population, such as the Alzheimer's Disease
Neuroimaging Initiative (ADNI) [@Weiner2012],
are designed to mitigate some of the relevant statistical issues.  Analogously, much
research has been devoted to exploring methodologies for properly exploiting such
studies and avoiding various forms of processing bias [@Reuter:2012aa].  For example,
FSL's SIENA (Structural Image Evaluation, using Normalization, of Atrophy) framework
[@Smith:2002aa] for detecting atrophy between two time points avoids a specific type
of processing bias by transforming the images to a midspace position between the two
time points.
As the authors point out "In this way both images are subjected to a similar degree
of interpolation-related blurring."  Consequences of this "interpolation-related
blurring" were formally analyzed in [@Yushkevich:2010aa] in the context of
hippocampal volumetric change where it was shown that interpolation-induced
artifacts can artificially inflate effect size [@Thompson:2011aa].  These insights
have since been used for making specific recommendations with respect to longitudinal
image data processing [@Fox:2011aa;@Reuter:2012aa;@Hua:2013aa].

In a series of papers [@Reuter:2011aa;@Reuter:2012aa] the authors
motivated the design and implementation of the longitudinal FreeSurfer variant partly
inspired by these earlier insights and encapsulated by the overarching heuristic
of "treat[ing] all time points exactly the same."  It has
since been augmented by integrated linear mixed effects modeling capabilities
[@Bernal-Rusiel:2013aa] and has been used in a variety of studies including pediatric
cortical development [@Wierenga:2014aa], differential development in Alzheimer's
disease and fronto-temporal dementia [@Landin-Romero:2016aa], and fatigue in the
context of multiple sclerosis [@Nourbakhsh:2016aa].

In [@Tustison:2014ab], we introduced the Advanced Normalization Tools (ANTs) cortical
thickness framework which leverages various pre-processing, registration, segmentation,
and other image analysis tools that members of the ANTs and Insight Toolkit (ITK)
open-source communities have developed over the years and disseminated publicly.[^1]
This proposed ANTs-based pipeline has since been directed at a variety of neuroimaging
research topics including mild cognitive impairment and depression [@Fujishima:2014aa],
short term memory in mild cognitive impairment [@Das:2016aa], and aphasia [@Olm:2016aa].
In this work, we introduce the longitudinal version of the ANTs cortical thickness
pipeline and demonstrate its utility on the publicly available ADNI-1 data set.  
We demonstrate that certain longitudinal processing choices have significant impact
on measurement quality in terms of within-subject and between subject variances which,
in turn, heavily impacts the scientifically interpretability of results.  Similar to
other research illustrating the negative impact of interpolation effects on study results,
we show that a common practice for unbiased processing induces a different set of
problematic artifacts which guides processing choices for the proposed ANTs pipeline.  In
addition, these choices for the ADNI-1 data produce tighter confidence intervals
in calculated mean trends, smaller prediction intervals,
and less varied confidence/credible intervals for discerning cross-sectional effects.

[^1]: https://github.com/stnava/ANTs

\newpage
# Methods and materials

## Longitudinal ADNI imaging data

![Demographic breakdown of the number of ADNI subjects by diagnosis i.e., normal,
mild cognitive impairment (MCI), late mild cognitive impairment (LMCI),
and Alzheimer's disease (AD).  Within each panel we plot the number of subjects
(by gender) per visit---baseline ("bl") and $n$ months ("m$n$").](../Figures/demoPlot.png)

The strict protocol design, large-scale recruitment, and public availability of
the Alzheimer’s Disease Neuroimaging Initiative (ADNI) makes it
an ideal data set for evaluating the ANTs longitudinal cortical thickness pipeline.
An MP-RAGE [@Mugler:1990aa] sequence for 1.5 and 3.0 T was used to collect the data
at the scan sites.  Specific acquisition parameters for 1.5 T and 3.0 T magnets
are given in Table 1 of [@Jack:2008aa].  Originally, collection goals were
200 elderly cognitively normal subjects collected at 0, 6, 12, 24, and 36 months;
400 MCI subjects at risk for AD conversion at 0, 6, 12, 18, 24, and 36 months; and
200 AD subjects at 0, 6, 12, and 24 months.  

![Age vs. Mini-mental examination (MMSE) scores for the ADNI subjects
by diagnosis.  ](../Figures/demoPlot2.png)

The ADNI-1 data was downloaded in May of 2014.  The data was first processed using
the ANTs cross-sectional cortical thickness pipeline [@Tustison:2014ab]
(4399 total images).  Data was then processed using the ANTs longitudinal
stream (described in the next section) only including data for which at least two
time points were available.  In the final set of csv files we only included
time points for which clinical scores (e.g., MMSE) were available.  In total,
we included 186 elderly cognitive normals, 178 MCI subjects, 128 LMCI subjects,
and 123 AD subjects.  A further breakdown of demographic information is given
in Figure X.  Similarly, in Figure X, we show the 2-D distribution of Age vs. mini-mental
examination (MMSE) scores taken at the month 12 visit across diagnoses for the subjects
analyzed.


## ANTs Cross-sectional processing

A thorough discussion of the ANTs cross-sectional thickness estimation framework
was previously discussed in [@Tustison:2014ab].  Briefly, given a T1-weighted brain MR image,
processing comprises the following five major steps (cf Figure 1 of [@Tustison:2014ab]):

* N4 bias correction [@Tustison:2010ac],
* brain extraction [@avants2010a],
* Atropos $n$-tissue segmentation [@Avants:2011aa], and
* cortical thickness estimation [@das2009].

ROI-based quantification is achieved through the use of the joint label fusion
approach of [@Wang:2013ab] and the use of the MindBoggle-101 data labeled using
the Desikan–Killiany–Tourville (DKT) protocol [@Klein:2012aa] consisting of 31
labels per hemisphere (cf Table 1).
This pipeline has since been enhanced by the implementation [@Tustison:2016aa] of a patch-based
denoising algorithm [@Manjon:2010aa] as an optional preprocessing step and multi-modal
integration capabilities (e.g., joint T1- and T2-weighted processing).  

\input{dktRegions.tex}

For evaluation, regional thickness statistics were calculated based on the DKT
parcellation scheme.  Test-retest error measurements were presented
from a cohort of 20 atlases taken from the OASIS data set which had been manually
labeled [@Klein:2012aa] and compared with the
analogous FreeSurfer thickness values.    Further evaluation employed a training/prediction
paradigm whereby DKT regional cortical thickness values generated from 1205
images taken from four publicly available data sets (i.e., IXI [@ixi], MMRR [@landman2011],
NKI [@nki], and OASIS [@oasis]) were used to predict age and gender using linear and
random forest models.  Although repeatability was comparable between the two packages,
predictive accuracy was improved with ANTs versus FreeSurfer.
The resulting regional statistics (including cortical thickness, surface area [@Lehmann:2012aa],
volumes, and Jacobian determinant values) are available online.[^2]  These include the corresponding
FreeSurfer measurements which are also publicly available for research
studies (e.g., [@Hasan:2016aa]).
Since publication this pipeline has been used in a number of cross-sectional studies
(e.g., [@Price:2015aa;@Wisse:2015aa;@Betancourt:2015aa]).

[^2]: https://github.com/ntustison/KapowskiChronicles



## Unbiased longitudinal processing

![Diagrammatic illustration of the ANTs longitudinal cortical thickness pipeline
for a single subject with $N$ time points.  From the $N$ original T1-weighted
images (left column, yellow panel) and the group template and priors (bottom row,
green panel), the single-single subject template (SST) and auxiliary prior images
are created (center, blue panel).  These subject-specific template and other
auxiliary images are used to
generate the individual time-point cortical thickness maps.  Optionally, one can
rigidly transform the time-point images prior to segmentation and cortical thickness
estimation (right column, red panel).  For regional thickness values, regional labels
can be propagated to each image using a given atlas set and cortical parcellation scheme.](../Figures/longitudinalPipeline.png)

Given certain practical limitations (e.g., subject recruitment and retainment),
as mentioned earlier, many researchers employ cross-sectional acquisition and
processing strategies for studying developmental phenomena.  Longitudinal
studies, on the other hand, can significantly reduce inter-subject measurement variability.
The ANTs longitudinal cortical thickness pipeline extends the ANTs cortical
thickness pipeline for longitudinal studies which takes into account various
bias issues previously discussed in the literature
[@Yushkevich:2010aa;@Reuter:2011aa;@Reuter:2012aa] and, to our knowledge,
interpolation effects not previously analyzed.
Given $N$ time-point T1-weighted MR images and a group template and prior
probability maps (described below), the longitudinal pipeline consists of the following steps:

1. Create single-subject template (SST).
2. Apply the ANTs cross-sectional pipeline to the SST.
3. Apply the results of 2) and the group template to create the SST prior probability maps.
4. (Optional):  Transform each individual time point to the SST.
5. Apply the ANTs cross-sectional pipeline to each individual time-point.
6. Use joint label fusion and the transforms obtained in steps 1)-5) to determine the cortical ROIS for statistical quantification.

An overview of these steps is provided in Figure X which we describe
in greater detail below.  One of the most significant findings presented below
is that the common step of transforming each individual
time point to the SST is suboptimal in that the corresponding interpolation
effects decrease the quality of cortical thickness measurements over
processing in native space.  

![Top row:  Canonical views of the template created from 52 cognitively normal subjects
of the ADNI database.  The prior probability mask for the whole brain (middle row)
and the 6 tissue priors (bottom row) are used to "seed" each SST during longitudinal
processing.](../Figures/adniTemplate2.png)

__ADNI group template.__  In general, prior to any subject processing, the group
template is constructed from the population data [@Avants:2010aa].  For the ADNI-1 processing
described in this work, we created a population-specific template from 52 cognitively normal subjects.
In addition, corresponding brain and tissue prior probability maps for the CSF, gray matter,
white matter,  deep gray matter, brain stem, and cerebellum were created as described
in [@Tustison:2014ab].  A brief overview of this is also provided in the next section
describing the single-subject template.
Canonical views of the template and auxiliary images are given
in Figure X.

__Single-subject template, brain mask, and tissue priors.__
Following the offline construction of the ADNI group template and prior probability images,
each subject undergoes identical processing.  First, an average shape and intensity single
subject template (SST) is created from all time point images [@Avants:2010aa] using the
same protocol used to produce the ADNI group template.
Subsequent processing segments
the SST into six probabilistic tissues classes:   cerebrospinal
fluid (CSF), gray matter (GM), white matter (WM), deep gray matter (striatum + thalamus),
brain stem, and cerebellum.  This requires processing the SST through two parallel workflows.  First,
the SST proceeds through the standard ANTs cortical thickness pipeline which generates
a brain extraction mask and the CSF posterior probability map.  Second, using
a data set of expert annotations [@Klein:2012aa], a multi-atlas joint label
fusion step [@Wang:2013ab] is performed to create individualized probability
maps for all tissue types.  The five JLF probabilistic tissue
estimates (GM, WM, deep GM, brain stem, and cerebellum)
are used as the SST prior probabilities after smoothing with a standard
deviation = 1 mm Gaussian kernel whereas the CSF SST prior probability
is derived as a combination of the JLF and segmentation CSF estimates, i.e.,
$P(CSF) = \max\left( P_{Seg}(CSF), P_{JLF}(CSF) \right)$, also smoothed
with the same smoothing operation.  This final version of the SST enables
unbiased mappings to the group template, subject-specific tissue segmentations, region of interest volumes and
cortical thickness maps for each of the original time series images.

__Individual time point processing.__
The T1-weighted image at each
time point is rigidly aligned to the template and processed through cross-sectional cortical
thickness pipeline using the SST template and auxiliary images (brain extraction mask and
tissue priors).  

  Each
time point image is then rigidly aligned to the SST.  The SST prior probability maps are
created using a protocol combining brain extraction and a six-tissue segmentation and
a six-label joint label fusion processing of the SST.  After the SST template priors are
created, each time point image is rigidly aligned to the template to reduce the
effect of coordinate system or interpolation bias.

__Pseudo-geodesic for large cohort labeling.__  The cortical ROIs
from the DKT atlases are propagated to each time point using a "pseudo-geodesic" mapping
and joint label fusion.

## Statistical methods

![](../Figures/allData.png)

![](../Figures/ratio.png)

We used a simple statistical principle to compare performance between
cross-sectional and longitudinal processing methods.  We said that one
processing method outperforms the other when it does a better job minimizing
within-subject variability and maximizing between-subject variability in
cortical thickness measurements.  Such a quality implies greater
within-subject reproducibility while distinguishing between patient
subpopulations. As such this will amount to higher precision when
cortical thickness is used as a predictor variable or model covariate in
statistical analyses upstream. This criterion is immediately assessable
in terms of estimates associated to the longitudinal
mixed-effects model \eqref{eq::lme1} outlined below.

Longitudinal mixed-effect (LME) models comprise a well-established and widely used
class of regression models designed to estimate cross-sectional and longitudinal
linear associations between quantities while accounting for subject specific
trends.  As such, these models are useful for the analysis of longitudinally
collected cohort data. Indeed, [@Bernal-Rusiel:2013aa] provide an introduction to the mixed-effects methodology in the context of longitudinal neuroimage data and compare it empirically to competing methods such as repeated measures ANOVA.
For more general, near comprehensive treatments of the subject matter, see [@verbeke2009linear] and [@fitzmaurice2012applied]. We claim that LME models are useful, not only for the analysis of longitudinal cohort data, but also for the comparison of techniques used
to obtain the data itself. By fitting simple LME models to the data
resulting from cross-sectional and longitudinal processing techniques,
we are able to make concrete such ideas as within-subject, between-subject,
and total variability in a way that [@reuter2012] only hint at in
their exposition of their longitudinal, FreeSurfer based methodology.

As previously noted we observed yearly cortical thickness measurements
from sixty-two separate regions of interest.  To assess the above
variability based criteria while accounting for changes that may occur through
the passage of time, we used a Bayesian LME model for parameter
estimation.  Let $Y^k_{ij}$ denote the $i^{th}$ individual's cortical
thickness measurement corresponding to the $k^{th}$ region of interest
at measurement $j$.  Under the Bayesian paradigm we utilized a model of
the form \begin{gather} Y^k_{ij} \sim N(\alpha^k_i + \beta^k t,
\sigma_k^2) \\ \nonumber \alpha^k_i \sim N(\alpha^k_0, \tau^2_k) \qquad
\alpha^k_0, \beta^k \sim N(0,10)  \qquad \sigma_k^2,  \tau_k^2 \sim
\mbox{Cauchy}^+ (0, 5) \end{gather}\label{eq::lme1} Specification of parameters in the
above prior distributions reflect commonly accepted diffuse priors.
$\tau^2_k$ represents the between-subject variance parameter, and
$\sigma^2_k$ represents the within-subject variance parameter.  For each
region, the quantity of interest is thus the ratio $r^k =
\frac{\tau^2_k}{\sigma^2_k}$.  This ratio is closely related to the
intraclass correlation coefficient [@bartko1976various]:
\begin{equation} \rho_I = \frac{\tau^2}{\tau^2 + \sigma^2}\ , \end{equation}
which is the temporally constant within-subject correlation of the
random intercepts model [@verbeke2009linear]. The posterior distribution of
$r^k$ was summarized via the posterior median. Where the posterior
distributions were obtained using Stan probabilistic programming
language [@carpenter2016stan].

![3-D volumetric rendering of the normed difference of the longitudinal variance ratio minus
the cross-sectional variance ratio specified for each of the 62 cortical regions.](../Figures/medianRatios3D.png)

For each processing method we performed sixty-two independent
regressions.  In order to compare results between methods, we considered
the quantity $\delta^k = r^k_l - r^k_c$ and $\delta^k_{norm} =
\frac{r^k_l - r^k_c}{r^k_l + r^k_c}$, denoting the variance ratio for
the longitudinal method minus that of the cross-sectional method and the
normed difference between ratios, respectively (cf Figure ??). Since a large $r^k$
implies a higher between-subject to within-subject variability ratio, a
positive estimate of $\delta^k$ that is large in magnitude implies that
the longitudinal processing method is preferable to the cross-sectional
method.  Conversely, a negative estimate that is large in magnitude
implies that the cross-sectional processing method is preferable to the
longitudinal method.

Finally, we used basic LME models and cortical thickness measurements of the entorhinal cortex to demonstrate how these variability criteria relate to potential scientific analyses. First, we used model \eqref{eq::lme1} to show that a greater ratio of between-subject to within-subject variability results in tighter confidence and credible intervals on the slope parameter $\beta$. This result indicates more confidence with respect to mean trends. Second, we showed that smaller within-subject variability corresponds to smaller prediction intervals when predicting a subject's cortical thickness levels at future visits. This is important when considering regional cortical thickness measures as candidate biomarkers.  Third, we extended model \eqref{eq::lme1} to include a term for AD diagnostic status and demonstrated that lower total variability corresponds to tighter confidence/credible intervals for cross-sectional effects. This corresponds to higher certainty when evaluating linear associations between quantities, such as cortical thickness and AD status.


## Effects of Interpolation on Thickness Results

Consistent with the heuristic cited in [@Reuter:2012aa] of "treat[ing] all
time points exactly the same", our original approach was to reorient all subject
time points to the subject-specific template to avoid any bias associated with
variations in head orientation.  Although this had an overall effect of increased between-subject
variance and decreased within-subject variance relative to cross-sectional processing,
there are several regions where this trend is reversed.  However, when each time
point was processed in its native space, there was no such trend reversal, i.e.
all regions for this longitudinal pipeline variant showed better ratio measurements
than both cross-sectional and reorientation-to-the-SST longitudinal processing.

Consistent with other studies that have demonstrated the detrimental effects of interpolation
(e.g., [@Yushkevich:2010aa]), we discovered that the interpolation associated with reorientation
to the SST adds a source of noise to the anatomical characteristics associated with thickness
and that this artificial change in anatomical geometry correlates regionally with both the
within-subject and between-subject variance measurements described above.  

To explore these effects in greater detail, we


This trend is illustrated in
Figures XX (within-subject variance) and YY (between-subject variance).

![Within-subject variance.](../Figures/correlationVolumeWithinSubjectVariance.png)

![Between-subject variance.](../Figures/correlationVolumeBetweenSubjectVariance.png)

# Results


# Discussion


## Subsection 1

And a sweet equation:

$$ \exp^{-i \pi} = -1 $$



\clearpage

# References
