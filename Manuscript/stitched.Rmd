---
output:
  pdf_document:
    fig_caption: true

  word_document:
    fig_caption: true
header-includes:
   - \usepackage{booktabs}
   - \usepackage[final]{changes}
   - \usepackage[font={small},labelfont=bf,labelsep=colon]{caption}
   - \linespread{1.5}
   - \usepackage{enumitem}
   - \usepackage{tikz}
   - \def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
   - \setlist{nolistsep}
   - \setremarkmarkup{(#2)}
bibliography: references.bib
csl: national-science-foundation-grant-proposals.csl
fontsize: 11pt
mainfont: Georgia
geometry: margin=1.0in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( cache=TRUE )
```

\pagenumbering{gobble}

<!--
---
output:
  word_document:
    fig_caption: true
  pdf_document:
    fig_caption: true
    latex_engine: xelatex
    keep_tex: yes
  html_document:
    toc: false
header-includes:
   - \usepackage{booktabs}
   - \usepackage[final]{changes}
   - \usepackage[font={small},labelfont=bf,labelsep=colon]{caption}
   - \linespread{1.2}
   - \usepackage[compact]{titlesec}
   - \usepackage{enumitem}
   - \usepackage{tikz}
   - \def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
   - \setlist{nolistsep}
   - \setremarkmarkup{(#2)}
bibliography: references.bib
csl: national-science-foundation-grant-proposals.csl
fontsize: 11pt
mainfont: Georgia
geometry: margin=1.0in
---
-->

\begin{centering}

$ $

\vspace{2.0 cm}

\LARGE

{\bf The ANTs Longitudinal Cortical Thickness Pipeline}

\vspace{0.25 cm}

\large
Nicholas J. Tustison$^{1,2}$,
Andrew J. Holbrook$^3$,
Brian B. Avants$^4$,
Jared M. Roberts$^2$,
Philip A. Cook$^5$,
James R. Stone$^1$,
Daniel L. Gillen$^3$, and
Michael A. Yassa$^2$
for the Alzheimer's Disease Neuroimaging Initiative*


\vspace{0.5 cm}

\small

$^1$Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA

$^2$Department of Neurobiology and Behavior, University of California, Irvine, Irvine, CA

$^3$Department of Statistics, University of California, Irvine, Irvine, CA

$^4$Biogen, Cambridge, MA

$^5$Department of Radiology, University of Pennsylvania, Philadelphia, PA

\end{centering}

\vspace{4.0 cm}

\small

Corresponding author: \
Nicholas J. Tustison \
211 Qureshey Research Lab \
Irvine, CA  92697-3800 \
540-383-2719 \
ntustison@virginia.edu \

\noindent\rule{4cm}{0.4pt}

\footnotesize
*Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf

\newpage

\normalsize

# Abstract

Large-scale longitudinal studies of developmental progression or disease in the human brain
have motivated the acquisition of large neuroimaging data sets and the
concomitant development of robust methodological and statistical tools
for insight into potential neurostructural changes.  Longitudinal-specific strategies
for acquisition and processing have potentially significant benefits including
the reduction of the inter-subject confound associated with cross-sectional
studies.  In this work, we introduce the open-source Advanced Normalization Tools
(ANTs) cortical thickness longitudinal processing pipeline and its application
on the first phase of the Alzheimer's Disease Neuroimaging Initiative (ADNI-1)
consisting of over 600 subjects with multiple time points from baseline to 36 months.
We demonstrate that the single-subject template construction and native subject-space
processing localizes data transformations and reduces interpolation artifacts, respectively,
and is the preferred processing strategy with respect to simultaneous minimization
of within-subject variability and maximization of between-subject variability, respectively.
It is further shown that optimizing these dual criteria
leads to greater scientific interpretability in terms of tighter confidence intervals
in calculated mean trends, smaller prediction intervals,
and tighter confidence/credible intervals for determining cross-sectional effects.

_Keywords:_  ANTs, Alzheimer's disease, bias, cortical thickness, interpolation, longitudinal processing

\clearpage

<!--

You can add internal comments which will not be reproduced using html comment delimiters.

-->

# Introduction

Quantification of brain morphology significantly facilitates the investigation of
a wide range of neurological conditions with structural correlates (e.g.,
Alzheimer's disease and frontotemporal dementia [@du2007;@dickerson2009],
Parkinson's disease [@jubault2011],
Williams syndrome [@thompson2005],
multiple sclerosis [@ramasamy2009],
autism [@chung2005,@hardan2006],
migraines [@dasilva2007],
chronic smoking [@kuhn2010],
alcoholism [@fortier2011],
cocaine addiction [@makris2008],
schizophrenia [@nesvag2008],
bipolar disorder [@lyoo2006],
autism [@chung2005,@hardan2006],
marijuana use in adolescents [@Jacobus:2015aa],
Tourette syndrome in children [@sowell2008],
scoliosis in female adolescents [@wang2012],
heart failure [@Kumar:2015aa],
early-onset blindness [@jiang2009],
chronic pancreatitis [@frokjaer2012],
obsessive-compulsive disorder [@shin2007],
ADHD [@almeida-montes2012],
obesity [@raji2010],
heritable [@peterson2009] and elderly [@ballmaier2004] depression,
age [@kochunov2011],
gender [@luders2006a],
untreated male-to-female transsexuality [@luders2012],
handedness
[@luders2006,amunts2007],
intelligence [@shaw2006],
athletic ability [@wei2011],
meditative practices [@lazar2005],
musical ability [@bermudez2009;@foster2010],
musical instrument playing [@Hudziak:2014aa],
tendency toward criminality [@raine2011],
childhood sexual abuse in adult females [@heim2013],
and Tetris-playing ability in female adolescents [@haier2009]).
Essential for thickness quantification are the
many computational techniques which have been developed
to provide accurate measurements of the cerebral cortex.
These include various mesh-based
(e.g., [@macdonald2000;@magnotta1999;@kim2005]) and
volumetric techniques
(e.g., [@zeng1999;@jones2000;@das2009;@clement-vachet2011]).
Of noted significance, and representing the former,
is the well-known and highly utilized FreeSurfer
software package [@dale1999;@fischl1999;@fischl2000;@fischl2002;@fischl2004].

In inferring developmental processes, many of these studies employ
cross-sectional population sampling strategies despite the potential for
confounding effects [@Kraemer:2000aa].  Large-scale studies involving longitudinal
image acquisition of a targeted subject population, such as the Alzheimer's Disease
Neuroimaging Initiative (ADNI) [@Weiner2012],
are designed to mitigate some of the relevant statistical issues.  Analogously, much
research has been devoted to exploring methodologies for properly exploiting such
studies and avoiding various forms of processing bias [@Reuter:2012aa].  For example,
FSL's SIENA (Structural Image Evaluation, using Normalization, of Atrophy) framework
[@Smith:2002aa] for detecting atrophy between two time points avoids a specific type
of processing bias by transforming the images to a midspace position between the two
time points.
As the authors point out "In this way both images are subjected to a similar degree
of interpolation-related blurring."  Consequences of this "interpolation-related
blurring" were formally analyzed in [@Yushkevich:2010aa] in the context of
hippocampal volumetric change where it was shown that interpolation-induced
artifacts can artificially inflate effect size [@Thompson:2011aa].  These insights
have since been used for making specific recommendations with respect to longitudinal
image data processing [@Fox:2011aa;@Reuter:2012aa;@Hua:2013aa].

In a series of papers [@Reuter:2011aa;@Reuter:2012aa] the authors
motivated the design and implementation of the longitudinal FreeSurfer variant partly
inspired by these earlier insights and encapsulated by the overarching heuristic
of "treat[ing] all time points exactly the same."  It has
since been augmented by integrated linear mixed effects modeling capabilities
[@Bernal-Rusiel:2013aa] and has been used in a variety of studies including pediatric
cortical development [@Wierenga:2014aa], differential development in Alzheimer's
disease and fronto-temporal dementia [@Landin-Romero:2016aa], and fatigue in the
context of multiple sclerosis [@Nourbakhsh:2016aa].

In [@Tustison:2014ab], we introduced the Advanced Normalization Tools (ANTs) cortical
thickness framework which leverages various pre-processing, registration, segmentation,
and other image analysis tools that members of the ANTs and Insight Toolkit (ITK)
open-source communities have developed over the years and disseminated publicly.[^1]
This proposed ANTs-based pipeline has since been directed at a variety of neuroimaging
research topics including mild cognitive impairment and depression [@Fujishima:2014aa],
short term memory in mild cognitive impairment [@Das:2016aa], and aphasia [@Olm:2016aa].
In this work, we introduce the longitudinal version of the ANTs cortical thickness
pipeline and demonstrate its utility on the publicly available ADNI-1 data set.  

We demonstrate that certain longitudinal processing choices have significant impact
on measurement quality in terms of within-subject and between subject variances which,
in turn, heavily impacts the scientifically interpretability of results.  Similar to
other research illustrating the negative impact of interpolation effects on study results,
we show that a common practice for unbiased processing induces a different set of
problematic artifacts which guides processing choices for the proposed ANTs pipeline.  In
addition, these choices for the ADNI-1 data produce tighter confidence intervals
in calculated mean trends, smaller prediction intervals,
and less varied confidence/credible intervals for discerning cross-sectional effects.

[^1]: https://github.com/stnava/ANTs

\newpage
# Methods and materials

## ADNI-1 imaging data

![Demographic breakdown of the number of ADNI-1 subjects by diagnosis i.e., normal,
mild cognitive impairment (MCI), late mild cognitive impairment (LMCI),
and Alzheimer's disease (AD).  Within each panel we plot the number of subjects
(by gender) per visit---baseline ("bl") and $n$ months ("m$n$").](../Figures/demoPlot.png)

The strict protocol design, large-scale recruitment, and public availability of
the Alzheimer’s Disease Neuroimaging Initiative (ADNI) makes it
an ideal data set for evaluating the ANTs longitudinal cortical thickness pipeline.
An MP-RAGE [@Mugler:1990aa] sequence for 1.5 and 3.0 T was used to collect the data
at the scan sites.  Specific acquisition parameters for 1.5 T and 3.0 T magnets
are given in Table 1 of [@Jack:2008aa].  Originally, collection goals were
200 elderly cognitively normal subjects collected at 0, 6, 12, 24, and 36 months;
400 MCI subjects at risk for AD conversion at 0, 6, 12, 18, 24, and 36 months; and
200 AD subjects at 0, 6, 12, and 24 months.  

![Age vs. Mini-mental examination (MMSE) scores for the ADNI-1 subjects
by diagnosis.  ](../Figures/demoPlot2.png)

The ADNI-1 data was downloaded in May of 2014.  The data was first processed using
the ANTs cross-sectional cortical thickness pipeline [@Tustison:2014ab]
(4399 total images).  Data was then processed using the ANTs longitudinal
stream (described in the next section).  In the final set of csv files we only included
time points for which clinical scores (e.g., MMSE) were available.  In total,
we included 186 elderly cognitive normals, 178 MCI subjects, 128 LMCI subjects,
and 123 AD subjects.  A further breakdown of demographic information is given
in Figure X.  Similarly, in Figure X, we show the 2-D distribution of Age vs. mini-mental
examination (MMSE) scores taken at the month 12 visit across diagnoses for the subjects
analyzed.

## ANTs cortical thickness


_Cross-sectional processing_

A thorough discussion of the ANTs cross-sectional thickness estimation framework
was previously discussed in [@Tustison:2014ab].  As a brief review, given a T1-weighted brain MR image,
processing comprises the following five major steps (cf Figure 1 of [@Tustison:2014ab]):

* N4 bias correction [@Tustison:2010ac],
* brain extraction [@avants2010a],
* Atropos $n$-tissue segmentation [@Avants:2011aa], and
* cortical thickness estimation [@das2009].

ROI-based quantification is achieved through the use of the joint label fusion
approach of [@Wang:2013ab] and the use of the MindBoggle-101 data labeled using
the Desikan–Killiany–Tourville (DKT) protocol [@Klein:2012aa] consisting of 31
labels per hemisphere (cf Table 1).
This pipeline has since been enhanced by the implementation [@Tustison:2016aa] of a patch-based
denoising algorithm [@Manjon:2010aa] as an optional preprocessing step and multi-modal
integration capabilities (e.g., joint T1- and T2-weighted processing).  

\input{dktRegions.tex}

For evaluation, regional thickness statistics were calculated based on the DKT
parcellation scheme.  Test-retest error measurements were presented
from a cohort of 20 atlases taken from the OASIS data set which had been manually
labeled [@Klein:2012aa] and compared with the
analogous FreeSurfer thickness values.    Further evaluation employed a training/prediction
paradigm whereby DKT regional cortical thickness values generated from 1205
images taken from four publicly available data sets (i.e., IXI [@ixi], MMRR [@landman2011],
NKI [@nki], and OASIS [@oasis]) were used to predict age and gender using linear and
random forest [@breiman2001] models.
The resulting regional statistics (including cortical thickness, surface area [@Lehmann:2012aa],
volumes, and Jacobian determinant values) were made available online.[^2]  These include the
corresponding FreeSurfer measurements which are also publicly available for research
studies (e.g., [@Hasan:2016aa]).
Since publication this pipeline has been used in a number of cross-sectional studies
(e.g., [@Price:2015aa;@Wisse:2015aa;@Betancourt:2015aa]).

[^2]: https://github.com/ntustison/KapowskiChronicles



_Unbiased longitudinal processing_

![Diagrammatic illustration of the ANTs longitudinal cortical thickness pipeline
for a single subject with $N$ time points.  From the $N$ original T1-weighted
images (left column, yellow panel) and the group template and priors (bottom row,
green panel), the single-single subject template (SST) and auxiliary prior images
are created (center, blue panel).  These subject-specific template and other
auxiliary images are used to
generate the individual time-point cortical thickness maps.  Optionally, one can
rigidly transform the time-point images prior to segmentation and cortical thickness
estimation (right column, red panel).  For regional thickness values, regional labels
can be propagated to each image using a given atlas set and cortical parcellation scheme.](../Figures/longitudinalPipeline.png)

Given certain practical limitations (e.g., subject recruitment and retainment),
as mentioned earlier, many researchers employ cross-sectional acquisition and
processing strategies for studying developmental phenomena.  Longitudinal
studies, on the other hand, can significantly reduce inter-subject measurement variability.
The ANTs longitudinal cortical thickness pipeline extends the ANTs cortical
thickness pipeline for longitudinal studies which takes into account various
bias issues previously discussed in the literature
[@Yushkevich:2010aa;@Reuter:2011aa;@Reuter:2012aa] and, to our knowledge,
interpolation effects not previously made explicit.

Given $N$ time-point T1-weighted MR images, a group template, and group template prior
probability maps (described below), the longitudinal pipeline consists of the following steps:

1. (Offline):  Creation of the group template.
2. Creation of the single-subject template (SST).
3. Application of the ANTs cross-sectional pipeline to the SST.
4. Creation of the SST prior probability maps.
5. (Optional):  Rigid transformation of each individual time point to the SST.
6. Application of the ANTs cross-sectional pipeline to each individual time-point image.
7. Joint label fusion to determine the cortical ROIs for analysis.

An overview of these steps is provided in Figure X which we describe
in greater detail below.  

<!--
One of the most significant findings presented below
is that the common step of transforming each individual
time point to the SST is suboptimal in that the corresponding interpolation
effects decrease the quality of cortical thickness measurements over
processing in native space.  
-->

![Top row:  Canonical views of the template created from 52 cognitively normal subjects
of the ADNI-1 database.  The prior probability mask for the whole brain (middle row)
and the six tissue priors (bottom row) are used to "seed" each single-subject template for creation of
a probabilistic brain mask and probabilistic tissues priors during longitudinal
processing.](../Figures/adniTemplate2.png)

__ADNI group template, brain mask, and tissue priors.__  Prior to any individual subject processing, the group
template is constructed from the population data [@Avants:2010aa].  For the ADNI-1 processing
described in this work, we created a population-specific template from 52 cognitively normal ADNI-1
subjects.
In addition, corresponding brain and tissue prior probability maps for the CSF, gray matter,
white matter,  deep gray matter, brain stem, and cerebellum were created as described
in [@Tustison:2014ab].  A brief overview of this process is also provided in the next section
describing the single-subject template.
Canonical views of the ADNI-1 template and corresponding auxiliary images are given
in Figure X.



__Single-subject template, brain mask, and tissue priors.__
Following the offline construction of the ADNI-1 group template and prior probability images,
each subject undergoes identical processing.  First, an average shape and intensity single
subject template (SST) is created from all time point images [@Avants:2010aa] using the
same protocol used to produce the ADNI-1 group template.
Next, six probabilistic tissue maps (cerebrospinal
fluid (CSF), gray matter (GM), white matter (WM), deep gray matter (striatum + thalamus),
brain stem, and cerebellum) are generated in the space of the SST.  This requires processing
the SST through two parallel workflows.  First,
the SST proceeds through the standard cross-sectional ANTs cortical thickness pipeline which generates
a brain extraction mask and the CSF tissue probability map, $P_{Seg}(CSF)$.  Second, using
a data set of 20 atlases from the OASIS data set that have been expertly annotated
[@Klein:2012aa], a multi-atlas joint label fusion step (JLF) [@Wang:2013ab] is performed
to create individualized probability
maps for all tissue types.  The five JLF probabilistic tissue
estimates (GM, WM, deep GM, brain stem, and cerebellum) and JLF CSF estimate,
$P_{JLF}(CSF)$,
are used as the SST prior probabilities after smoothing with a Gaussian kernel
($\sigma = 1 mm$) whereas the CSF SST tissue probability
is derived as a combination of the JLF and segmentation CSF estimates, i.e.,
$P(CSF) = \max\left( P_{Seg}(CSF), P_{JLF}(CSF) \right)$, also smoothed
with the same Gaussian kernel.  Finally, $P(CSF)$ is subtracted out from the other
five tissue probability maps.  The final version of the SST and auxiliary images enable
unbiased mappings to the group template, subject-specific tissue segmentations, region of interest volumes and
cortical thickness maps for each of the original time series images.

__Individual time point processing.__ In the FreeSurfer longitudinal stream, each time-point image is processed using
the FreeSurfer cross-sectional stream.  The resulting processed data from all time points
is then used to create a mean, or median, single-subject template.  Following
template creation, each time-point image is rigidly transformed to the template space where
it undergoes further processing (e.g., white and pial surface deformation).  This
reorientation to the template space "further reduce[s] variability" and permits an
"implicit vertex correspondence" across all time points [@Reuter:2012aa].

The ANTs longitudinal workflow shares some common aspects of its FreeSurfer
analog but differs in others as outlined above.  The first step for subject-wise
processing involves the creation of an optimal mean shape/intensity template
from all the time points [@Avants:2010aa].  For the cross-sectional ANTs processing,
the group template and auxiliary images are used to perform tasks such as individual
brain extraction and $n$-tissue segmentation prior to cortical thickness estimation [@Tustison:2014ab].
However, for the longitudinal variant, the group template is used to create the
SST auxiliary images.  We then map the SST and corresponding probabilistic tissue maps
to the native space of each time point where segmentation and cortical thickness is
estimated.  Note that this unbiased longitudinal pipeline is completely agnostic concerning ordering of
the input time-point images, i.e. we "treat all time points exactly the same."

During the initial development of this work, it was thought that rotating the
individual time points to the SST would be of benefit, similar to FreeSurfer, in reducing variability,
minimizing or eliminating possible orientation bias, and permitting a 4-D
segmentation given that the underlying Atropos implementation is dimensionality-agnostic
[@Avants:2011aa].  Regarding the latter, the possible benefit is potentially
outweighed by the possibility of "over-regularization" [@Reuter:2012aa] whereby
smoothing across time reduces detection ability of large time point changes.
Additionally, it is less than straightforward to accommodate irregular temporal sampling
such as the acquisition schedule of the ADNI-1 protocol.

Most significantly, though, reorienting each time point image to the SST has
significant detrimental measurement effects in which interpolation bias induces
artificial anatomical changes and these changes correlate significantly with specific
regions.  Since we are measuring the thickness of the highly convoluted cortex
involving measurements on the order of $2-8 mm$ from images with $\sim 1 mm^3$
voxels, these artificial changes significantly effect clinically related
criteria of measurement quality such as confidence intervals and predictability.
This is discussed further in Results.

__Joint label fusion and pseudo-geodesic for large cohort labeling.__  Cortical
thickness ROI-based analyses are performed using joint label fusion [@Wang:2013ab]
and whatever cortical parcellation scheme is deemed appropriate for the specific
study.  The brute force application of the joint label fusion algorithm would
require $N$ pairwise registrations for each time point image where N is the
number of atlases used.  This would require a significant computational cost for
a relatively large study such as ADNI.  Instead, we use the "pseudo-geodesic" approach
for mapping atlases to individual time point images.  The transformations between
the atlas and the group template are computed offline.  With that set of transforms,
we are able to concatenate a set of existing transforms from each atlas through
the group template, to the SST, and finally to each individual time point.

## Statistical methods

In [@Tustison:2014ab] we evaluated the performance of the ANTs cross-sectional pipeline
via a thorough comparison with FreeSurfer involving both repeatability and
demographic predictability criteria for a large-scale, widely varied data set.  Both
criteria demonstrated the significance of that work for estimating cortical thickness.
We use this established performance to further compare the longitudinal variants
described in this work.  

_Within-subject/between-subject variability_

We used a simple statistical principle to compare performance between
cross-sectional and longitudinal processing methods.  We said that one
processing method outperforms the other when it does a better job minimizing
within-subject variability and maximizing between-subject variability in
cortical thickness measurements.  Such a quality implies greater
within-subject reproducibility while distinguishing between patient
subpopulations. As such this will amount to higher precision when
cortical thickness is used as a predictor variable or model covariate in
statistical analyses upstream. This criterion is immediately assessable
in terms of estimates associated to the longitudinal
mixed-effects model \eqref{eq::lme1} outlined below.

Longitudinal mixed-effect (LME) models comprise a well-established and widely used
class of regression models designed to estimate cross-sectional and longitudinal
linear associations between quantities while accounting for subject specific
trends.  As such, these models are useful for the analysis of longitudinally
collected cohort data. Indeed, [@Bernal-Rusiel:2013aa] provide an introduction to the mixed-effects methodology in the context of longitudinal neuroimage data and compare it empirically to competing methods such as repeated measures ANOVA.
For more general, near comprehensive treatments of the subject matter, see [@verbeke2009linear] and [@fitzmaurice2012applied]. We claim that LME models are useful, not only for the analysis of longitudinal cohort data, but also for the comparison of techniques used
to obtain the data itself. By fitting simple LME models to the data
resulting from cross-sectional and longitudinal processing techniques,
we are able to make concrete such ideas as within-subject, between-subject,
and total variability in a way that [@reuter2012] only hint at in
their exposition of their longitudinal, FreeSurfer based methodology.

_Bayesian LME modeling for parameter estimation_

As previously noted we observed yearly cortical thickness measurements
from sixty-two separate regions of interest.  To assess the above
variability based criteria while accounting for changes that may occur through
the passage of time, we used a Bayesian LME model for parameter
estimation.  Let $Y^k_{ij}$ denote the $i^{th}$ individual's cortical
thickness measurement corresponding to the $k^{th}$ region of interest
at measurement $j$.  Under the Bayesian paradigm we utilized a model of
the form \begin{gather} Y^k_{ij} \sim N(\alpha^k_i + \beta^k t,
\sigma_k^2) \\ \nonumber \alpha^k_i \sim N(\alpha^k_0, \tau^2_k) \qquad
\alpha^k_0, \beta^k \sim N(0,10)  \qquad \sigma_k^2,  \tau_k^2 \sim
\mbox{Cauchy}^+ (0, 5) \end{gather}\label{eq::lme1} Specification of parameters in the
above prior distributions reflect commonly accepted diffuse priors.
$\tau^2_k$ represents the between-subject variance parameter, and
$\sigma^2_k$ represents the within-subject variance parameter.  For each
region, the quantity of interest is thus the ratio $r^k =
\frac{\tau^2_k}{\sigma^2_k}$.  This ratio is closely related to the
intraclass correlation coefficient [@bartko1976various]:
\begin{equation} \rho_I = \frac{\tau^2}{\tau^2 + \sigma^2}\ , \end{equation}
which is the temporally constant within-subject correlation of the
random intercepts model [@verbeke2009linear]. The posterior distribution of
$r^k$ was summarized via the posterior median. Where the posterior
distributions were obtained using Stan probabilistic programming
language [@carpenter2016stan].


For each processing method we performed sixty-two independent
regressions.  In order to compare results between methods, we considered
the quantity $\delta^k = r^k_l - r^k_c$ and $\delta^k_{norm} =
\frac{r^k_l - r^k_c}{r^k_l + r^k_c}$, denoting the variance ratio for
the longitudinal method minus that of the cross-sectional method and the
normed difference between ratios, respectively (cf Figure ??). Since a large $r^k$
implies a higher between-subject to within-subject variability ratio, a
positive estimate of $\delta^k$ that is large in magnitude implies that
the longitudinal processing method is preferable to the cross-sectional
method.  Conversely, a negative estimate that is large in magnitude
implies that the cross-sectional processing method is preferable to the
longitudinal method.

_LME modeling of entorhinal cortical thickness_

We used basic LME models and cortical thickness measurements of the entorhinal cortex to demonstrate how these variability criteria relate to potential scientific analyses. First, we used model \eqref{eq::lme1} to show that a greater ratio of between-subject to within-subject variability results in tighter confidence and credible intervals on the slope parameter $\beta$. This result indicates more confidence with respect to mean trends. Second, we showed that smaller within-subject variability corresponds to smaller prediction intervals when predicting a subject's cortical thickness levels at future visits. This is important when considering regional cortical thickness measures as candidate biomarkers.  Third, we extended model \eqref{eq::lme1} to include a term for AD diagnostic status and demonstrated that lower total variability corresponds to tighter confidence/credible intervals for cross-sectional effects. This corresponds to higher certainty when evaluating linear associations between quantities, such as cortical thickness and AD status.


_Evaluation based on diagnostic prediction_

In addition to the evaluation employing the between-subject/within-subject variance
criterion, we also used a training/prediction evaluation paradigm.  We assumed a
crude measurement of regional thickness change for predicting diagnosis:  
cognitively normal, mild cognitive impairment (MCI), late mild cognitive impairment (LMCI),
and Alzheimer's disease (AD).  Since each subject maintained a constant diagnosis
through all image acquisition visits, we simply fit a line to the thickness
measurement of each of the 62 regions to determine the set regional rates of change (i.e., slope)
for that subject. We then split the set of subject data into training and testing data sets (90\%
for the former and 10\% for the latter) for 500 permutations.  For each permutation, we  
construct three prediction models for each of the three pipelines ("Cross", "Long1", and "Long2")
using extreme gradient boosting and subsequently compute the models' accuracy from the resulting
4-class confusion matrices.  This provides a clinically-based assessment of measurement
quality.

_Effects of interpolation on thickness results_

Consistent with the heuristic cited in [@Reuter:2012aa] of "treat[ing] all
time points exactly the same", our original longitudinal approach was to reorient all subject
time points to the subject-specific template to avoid any bias associated with
variations in head orientation and reduce variability in processing.  
Although this had an overall effect of increased between-subject
variance and decreased within-subject variance relative to cross-sectional processing,
there are several regions where this trend is reversed (e.g., left and right
entorhinal cortex).  However, altering this workflow such that each time
point is processed in its native space, there is no such trend reversal, i.e.
all regions for this longitudinal pipeline variant showed better between-subject,
within-subject variability ratio measurements
than both cross-sectional and reorientation-to-the-SST longitudinal processing.

Consistent with other studies that have demonstrated the detrimental effects of interpolation
(e.g., [@Yushkevich:2010aa]), we discovered that the interpolation associated with reorientation
to the SST induces a significant change in the anatomical measurements associated with thickness
and that this artificial change in anatomical geometry correlates regionally with both the
within-subject and between-subject variance measurements described above.  This phenomenon
is analyzed more formally in the next section.

# Results

## Comparison of ANTs cortical thickness pipelines

As described previously, we employ a number of criteria to evaluate the performance
of the ANTs cortical thickness longitudinal pipeline  with the original cross-sectional
workflow described previously [@Tustison:2014ab].  Further, we look at two variants
of the longitudinal pipeline:  the first where each time point image is reoriented to the
SST prior to segmentation and cortical thickness estimation (denoted as "longitudinal 1")
and the second where each time point is processed in native space (denoted as
 "longitudinal 2").  

 _Within-subject/between-subject variability_

Maximizing the between-subject variability while simultaneously minimizing the
within-subject variability minimizes the overlap between groups implying a more
significant effect of the condition separating the groups.  In Figure X, we provide
a general overview of how each of these three measurements compare across
the three different ANTs pipeline variants over the 62 DKT regions.  This illustrates
the superiority of the longitudinal 2 pipeline where processing occurs in native
space.  It is also interesting to note that longitudinal 1 is an improvement over
the cross-sectional processing in that, overall, it reduces both types of variability.

![Notched box plots showing the distribution of the within-subject variability,
between subject variability, and ratio of the between-subject variability and
within-subject variability for each of the 62 DKT regions.  Note that the
"better" measurement maximizes this latter ratio.](../Figures/allData.png)

Further insights can be gleaned by looking at the ratio measurements per region
(see Figure X).  On a per region basis, longitudinal 2 outperforms both the cross-sectional
and longitudinal 1 pipelines.  Although longitudinal 1 is superior for the majority
of the DKT regions over cross-sectional processing, exceptions include the
left and right entorhinal,
left and right fusiform,
left and right inferior temporal,
left and right paracentral,
left and right parahippocampal,
left and right rosterior anterior cingulate,
left lateral orbitofrontal,
left medial orbitofrontal,
left pars triangularis,
right cuneus,
right insula,
right isthmus cingulate,
right pericalcarine,
right posterior cingulate
right precentral,
right postcentral, and
right transverse temporal.

![Per region quantities of the variance ratio (between-subject and within subject
 variability).  These values are plotted spatially in Figure X.](../Figures/ratio.png)

![3-D volumetric rendering of the variance ratio values plotted in
Figure X.](../Figures/medianRatios3D.png)

_Clinically-based assessment of measurement quality_   

A clinically-based prediction strategy was performed to evaluate the quality
of the cortical thickness measurements produced by each pipeline.  The rate of
thickness change determined over the set of subject imaging visits was used
as a feature set for predicting diagnosis.  The basic idea is that regional
thinning is accelerated in some regions versus others which should be reflected
in the assigned diagnostic category.

![Density plot of the accuracy for each of the three pipeline choices.  Accuracy was
 determined from the confusion matrices that were calculated for each of the
 500 iterations.  Using Tukey multiple comparisons of means at a 95% family-wise
 confidence level, the adjusted p-values were: Long1 $-$ Cross = 0.067,
 Long2 $-$ Cross < 1e-6, Long2 $-$ Long1 = 0.00019.](../Figures/accuracy0.9.png)

For each of 500 permutations, an extreme gradient boosting model [@xgboost]
was constructed from 90\% of the cortical thickness slope data corresponding to each
pipeline.  Based on previous experience, parameters of these models were chosen
from the default parameters which included a gradient step of 0.6 and 100 iterations
(or trees).
The resulting models were then used to predict on the remaining 10\%
of the data.  The resulting accuracy distributions are plotted in Figure X and
compared statistically using Tukey's range test which indicated increasing
performance Cross-sectional < Longitudinal 1 < Longitudinal 2.  These models
also provide means for assessing feature "importance" through the "Gain" values
which describes "the improvement in accuracy brought by a feature to the branches
[of the tree or iteration] it is on."

 ![The "gain" measurements of the extreme gradient boosting models for the
 Longitudinal 2 processing.](../Figures/importanceCombinedLong20.9.png)

_Interpolation effects associated with longitudinal 1 processing_

![The between-subject variability (left) and within-subject variability (right)
versus the volumetric (top) and surface area (bottom) measurements.
Note that each of these correlations are significant ($p < 0.001$)](../Figures/oasisCorrelationVolumeAndSurfaceArea.png)

The subset of regions where the longitudinal 1 $<$ cross-sectional in terms of
the variance ratio caused us to investigate this issue further.  Considering the
only difference between longitudinal 1 and longitudinal 2 is the reorientation
to the SST, our exploration focused on the effects of interpolation. Specifically,
we suspected that interpolation artificially changes the anatomy (i.e., volume
and surface area) which results in additive  noise to the thickness measurements
and that this effect varies spatially.  To test this, we used the 20 DKT atlases
[@klein2012] which were sampled from the OASIS data set.  An optimal mean/shape
template [@Avants:2010aa] was created from this cohort to which each T1-weighted
image was rigidly registered.  Using the resulting rigid transform, the label
map of 62 regions was warped to the space of the template (similar in spirit to
the protocol characterizing longitudinal 1) using nearest neighbor interpolation
(check on this).  Calculation of the volumes (by summing up the volume of each
voxel) and surface areas (using a well-performing digitally-based surface estimator
[@Lehmann:2012aa]) for each of the 62 regions were calculated both before and after
transformation to the template.  We then computed the percent change for each
of the two anatomical measures and correlated those with the per subject variability
which is plotted in Figure X.

\newpage

# Discussion

\newpage

## Acknowledgments

Data collection and sharing for this project was funded by the Alzheimer's
Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant
U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012).
ADNI is funded by the National Institute on Aging, the National Institute of
Biomedical Imaging and Bioengineering, and through generous contributions from
the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery
Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb
Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli
Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated
company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer
Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical
Research & Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale
Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis
Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda
Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes
of Health Research is providing funds to support ADNI clinical sites in Canada.
Private sector contributions are facilitated by the Foundation for the National
Institutes of Health (www.fnih.org). The grantee organization is the Northern
California Institute for Research and Education, and the study is coordinated
by the Alzheimer’s Therapeutic Research Institute at the University of Southern
California. ADNI data are disseminated by the Laboratory for Neuro Imaging at
the University of Southern California.

\newpage

# References
